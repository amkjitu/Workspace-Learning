{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['RiceBacterialBlight' 'RiceBacterialBlight' 'RiceBacterialBlight'\n",
      " 'RiceBacterialBlight' 'RiceBacterialBlight' 'RiceBacterialBlight'\n",
      " 'RiceBacterialBlight' 'RiceBacterialBlight' 'RiceBacterialBlight'\n",
      " 'RiceBacterialBlight' 'RiceBacterialBlight' 'RiceBacterialBlight'\n",
      " 'RiceBacterialBlight' 'RiceBacterialBlight' 'RiceBacterialBlight'\n",
      " 'RiceBacterialBlight' 'RiceBacterialBlight' 'RiceBacterialBlight'\n",
      " 'RiceBacterialBlight' 'RiceBacterialBlight' 'RiceBacterialBlight'\n",
      " 'RiceBacterialBlight' 'RiceBacterialBlight' 'RiceBacterialBlight'\n",
      " 'RiceBacterialBlight' 'RiceBacterialBlight' 'RiceBacterialBlight'\n",
      " 'RiceBacterialBlight' 'RiceBacterialBlight' 'RiceBacterialBlight'\n",
      " 'RiceBacterialBlight' 'RiceBacterialBlight' 'RiceBacterialBlight'\n",
      " 'RiceBacterialBlight' 'RiceBacterialBlight' 'RiceBacterialBlight'\n",
      " 'RiceBlast' 'RiceBlast' 'RiceBlast' 'RiceBlast' 'RiceBlast' 'RiceBlast'\n",
      " 'RiceBlast' 'RiceBlast' 'RiceBlast' 'RiceBlast' 'RiceBlast' 'RiceBlast'\n",
      " 'RiceBlast' 'RiceBlast' 'RiceBlast' 'RiceBlast' 'RiceBlast' 'RiceBlast'\n",
      " 'RiceBlast' 'RiceBlast' 'RiceBlast' 'RiceBlast' 'RiceBlast' 'RiceBlast'\n",
      " 'RiceBlast' 'RiceBlast' 'RiceBlast' 'RiceBlast' 'RiceBlast' 'RiceBlast'\n",
      " 'RiceBlast' 'RiceBlast' 'RiceBlast' 'RiceBlast' 'RiceBlast' 'RiceBlast'\n",
      " 'RiceBlast' 'RiceBlast' 'RiceBlast' 'RiceBlast' 'RiceBlast' 'RiceBlast'\n",
      " 'RiceBlast' 'RiceBlast' 'RiceBlast' 'RiceBlast' 'RiceBlast' 'RiceBlast'\n",
      " 'RiceBrownSpot' 'RiceBrownSpot' 'RiceBrownSpot' 'RiceBrownSpot'\n",
      " 'RiceBrownSpot' 'RiceBrownSpot' 'RiceBrownSpot' 'RiceBrownSpot'\n",
      " 'RiceBrownSpot' 'RiceBrownSpot' 'RiceBrownSpot' 'RiceBrownSpot'\n",
      " 'RiceBrownSpot' 'RiceBrownSpot' 'RiceBrownSpot' 'RiceBrownSpot'\n",
      " 'RiceBrownSpot' 'RiceBrownSpot' 'RiceBrownSpot' 'RiceBrownSpot'\n",
      " 'RiceBrownSpot' 'RiceBrownSpot' 'RiceBrownSpot' 'RiceBrownSpot'\n",
      " 'RiceBrownSpot' 'RiceBrownSpot' 'RiceBrownSpot' 'RiceBrownSpot'\n",
      " 'RiceBrownSpot' 'RiceBrownSpot' 'RiceBrownSpot' 'RiceBrownSpot'\n",
      " 'RiceBrownSpot' 'RiceBrownSpot' 'RiceBrownSpot' 'RiceBrownSpot'\n",
      " 'RiceBrownSpot' 'RiceBrownSpot' 'RiceBrownSpot' 'RiceBrownSpot'\n",
      " 'RiceBrownSpot' 'RiceBrownSpot' 'RiceBrownSpot' 'RiceSheathBrownRot'\n",
      " 'RiceSheathBrownRot' 'RiceSheathBrownRot' 'RiceSheathBrownRot'\n",
      " 'RiceSheathBrownRot' 'RiceSheathBrownRot' 'RiceSheathBrownRot'\n",
      " 'RiceSheathBrownRot' 'RiceSheathBrownRot' 'RiceSheathBrownRot'\n",
      " 'RiceSheathBrownRot' 'RiceSheathBrownRot' 'RiceSheathBrownRot'\n",
      " 'RiceSheathBrownRot' 'RiceSheathBrownRot' 'RiceSheathBrownRot'\n",
      " 'RiceSheathBrownRot' 'RiceSheathBrownRot' 'RiceSheathBrownRot'\n",
      " 'RiceSheathBrownRot' 'RiceSheathBrownRot' 'RiceSheathBrownRot'\n",
      " 'RiceSheathBrownRot' 'RiceSheathBrownRot']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Get the path to the dataset folder.\n",
    "dataset_dir = \"C:/Users/HP 840 G1/Documents/VS Code Projects/Workspace Learning/DataScience/Jupyter Notebook/Thesis_CSE/datasets/Rice_Leaf_Disease_Images_Dataset-4\"\n",
    "\n",
    "# Get the list of diseases.\n",
    "diseases = os.listdir(dataset_dir)\n",
    "\n",
    "# Create the lists of training and validation images and labels.\n",
    "train_images = []\n",
    "train_labels = []\n",
    "val_images = []\n",
    "val_labels = []\n",
    "\n",
    "# Iterate over the diseases.\n",
    "for disease in diseases:\n",
    "    # Get the path to the folder of the disease.\n",
    "    disease_dir = os.path.join(dataset_dir, disease)\n",
    "\n",
    "    # Iterate over the images in the folder.\n",
    "    for image_name in os.listdir(disease_dir):\n",
    "        # Get the path to the image.\n",
    "        image_path = os.path.join(disease_dir, image_name)\n",
    "\n",
    "        # Read the image.\n",
    "        image = cv2.imread(image_path)\n",
    "\n",
    "        # Randomly assign the image to the training or validation set.\n",
    "        split = np.random.rand()\n",
    "        if split < 0.8:\n",
    "            train_images.append(image)\n",
    "            train_labels.append(disease)\n",
    "        else:\n",
    "            val_images.append(image)\n",
    "            val_labels.append(disease)\n",
    "\n",
    "# Convert the lists of images and labels to NumPy arrays.\n",
    "train_images = np.array(train_images)\n",
    "train_labels = np.array(train_labels)\n",
    "val_images = np.array(val_images)\n",
    "val_labels = np.array(val_labels)\n",
    "\n",
    "datasets = (train_images, train_labels), (val_images, val_labels)\n",
    "\n",
    "print(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "151\n",
      "151\n",
      "32\n",
      "32\n",
      "Epoch 1/40\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"c:\\Program Files\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1332, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Program Files\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1316, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Program Files\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1297, in run_step  **\n        outputs = model.train_step(data)\n    File \"c:\\Program Files\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1073, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"c:\\Program Files\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1131, in compute_loss\n        return self.compiled_loss(\n    File \"c:\\Program Files\\Python311\\Lib\\site-packages\\keras\\src\\engine\\compile_utils.py\", line 265, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"c:\\Program Files\\Python311\\Lib\\site-packages\\keras\\src\\losses.py\", line 142, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"c:\\Program Files\\Python311\\Lib\\site-packages\\keras\\src\\losses.py\", line 268, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"c:\\Program Files\\Python311\\Lib\\site-packages\\keras\\src\\losses.py\", line 2122, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"c:\\Program Files\\Python311\\Lib\\site-packages\\keras\\src\\backend.py\", line 5560, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (None,) and (None, 4, 10, 4) are incompatible\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 120\u001b[0m\n\u001b[0;32m    116\u001b[0m     cv2\u001b[39m.\u001b[39mwaitKey(\u001b[39m0\u001b[39m)\n\u001b[0;32m    119\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m--> 120\u001b[0m     main()\n",
      "Cell \u001b[1;32mIn[3], line 96\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mlen\u001b[39m(x_test))\n\u001b[0;32m     94\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mlen\u001b[39m(y_test))\n\u001b[1;32m---> 96\u001b[0m model\u001b[39m.\u001b[39;49mfit( x_train, y_train, validation_data\u001b[39m=\u001b[39;49m(x_test, y_test),\n\u001b[0;32m     97\u001b[0m epochs\u001b[39m=\u001b[39;49m\u001b[39m40\u001b[39;49m,)\n\u001b[0;32m     99\u001b[0m model\u001b[39m.\u001b[39msave(\u001b[39m\"\u001b[39m\u001b[39mmodel.h5\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    100\u001b[0m model\u001b[39m.\u001b[39mload_weights(\u001b[39m'\u001b[39m\u001b[39mmodel.h5\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Program Files\\Python311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mC:\\Users\\HP840G~1\\AppData\\Local\\Temp\\__autograph_generated_filegugjocxk.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(step_function), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m), ag__\u001b[39m.\u001b[39mld(iterator)), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"c:\\Program Files\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1332, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Program Files\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1316, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Program Files\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1297, in run_step  **\n        outputs = model.train_step(data)\n    File \"c:\\Program Files\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1073, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"c:\\Program Files\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1131, in compute_loss\n        return self.compiled_loss(\n    File \"c:\\Program Files\\Python311\\Lib\\site-packages\\keras\\src\\engine\\compile_utils.py\", line 265, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"c:\\Program Files\\Python311\\Lib\\site-packages\\keras\\src\\losses.py\", line 142, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"c:\\Program Files\\Python311\\Lib\\site-packages\\keras\\src\\losses.py\", line 268, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"c:\\Program Files\\Python311\\Lib\\site-packages\\keras\\src\\losses.py\", line 2122, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"c:\\Program Files\\Python311\\Lib\\site-packages\\keras\\src\\backend.py\", line 5560, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (None,) and (None, 4, 10, 4) are incompatible\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "\n",
    "def faster_rcnn(image_size, num_classes, num_landmarks):\n",
    "    \"\"\"\n",
    "    Creates a Faster R-CNN model with ResNet50 backbone and landmark regression layer.\n",
    "\n",
    "    Args:\n",
    "      image_size: The size of the input images.\n",
    "      num_classes: The number of object classes.\n",
    "      num_landmarks: The number of landmarks.\n",
    "\n",
    "    Returns:\n",
    "      A Keras model.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create the ResNet50 model.\n",
    "    resnet50 = ResNet50(include_top=False, weights='imagenet')\n",
    "\n",
    "    # Create the RPN.\n",
    "    rpn_conv2d = Conv2D(256, (3, 3), padding='same')(resnet50.output)\n",
    "    rpn_output = tf.keras.layers.Dense(4)(rpn_conv2d)\n",
    "\n",
    "    # Create the Fast R-CNN network.\n",
    "    fast_rcnn_conv2d = Conv2D(128, (3, 3), padding='same')(rpn_conv2d)\n",
    "    fast_rcnn_output = tf.keras.layers.Dense(num_classes)(fast_rcnn_conv2d)\n",
    "\n",
    "    # Create the landmark regression layer.\n",
    "    landmark_regression = Conv2D(\n",
    "        2 * num_landmarks, (3, 3), padding='same')(fast_rcnn_conv2d)\n",
    "\n",
    "    # Create the model.\n",
    "    model = Model(inputs=resnet50.input, outputs=[\n",
    "                  rpn_output, fast_rcnn_output, landmark_regression])\n",
    "\n",
    "    return model\n",
    "\n",
    "def draw_bounding_box(image, bounding_box):\n",
    "    \"\"\"\n",
    "    Draws a bounding box on the image.\n",
    "\n",
    "    Args:\n",
    "      image: The input image.\n",
    "      bounding_box: Bounding box coordinates (x1, y1, x2, y2).\n",
    "\n",
    "    Returns:\n",
    "      An image with the bounding box drawn.\n",
    "    \"\"\"\n",
    "    x1, y1, x2, y2 = bounding_box\n",
    "    image_with_box = image.copy()\n",
    "    cv2.rectangle(image_with_box, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "    return image_with_box\n",
    "\n",
    "def draw_landmarks(image, landmarks):\n",
    "    \"\"\"\n",
    "    Draws landmarks on the image.\n",
    "\n",
    "    Args:\n",
    "      image: The input image.\n",
    "      landmarks: Landmark coordinates.\n",
    "\n",
    "    Returns:\n",
    "      An image with landmarks drawn.\n",
    "    \"\"\"\n",
    "    image_with_landmarks = image.copy()\n",
    "    for landmark in landmarks:\n",
    "        x, y = landmark\n",
    "        cv2.circle(image_with_landmarks, (x, y), 5, (255, 0, 0), -1)\n",
    "    return image_with_landmarks\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Load the dataset.\n",
    "    (x_train, y_train), (x_test, y_test) = datasets\n",
    "    \n",
    "    # Preprocess the images.\n",
    "    x_train = x_train.astype('float32') / 255.0\n",
    "    x_test = x_test.astype('float32') / 255.0\n",
    "\n",
    "    # Load the model.\n",
    "    model = faster_rcnn(image_size=(224, 224), num_classes=4, num_landmarks=4)\n",
    "\n",
    "    optimizer = Adam(learning_rate=0.0001)\n",
    "    model.compile(\n",
    "    optimizer=optimizer, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "    print(len(x_train))\n",
    "    print(len(y_train))\n",
    "    print(len(x_test))\n",
    "    print(len(y_test))\n",
    "\n",
    "    model.fit( x_train, y_train, validation_data=(x_test, y_test),\n",
    "    epochs=40,)\n",
    "\n",
    "    model.save(\"model.h5\")\n",
    "    model.load_weights('model.h5')\n",
    "\n",
    "    # Load the image.\n",
    "    image = cv2.imread('./datasets/Rice_Leaf_Disease_Images_Dataset-4/RiceBacterialBlight/BB (50).jpg')\n",
    "\n",
    "    # Predict the bounding box coordinates and landmarks.\n",
    "    predictions = model.predict(image[np.newaxis, ...])\n",
    "    bounding_box = predictions[0][0]\n",
    "    #landmarks = predictions[0][2]\n",
    "\n",
    "    # Draw the bounding box and landmarks on the image.\n",
    "    image = draw_bounding_box(image, bounding_box)\n",
    "    #image = draw_landmarks(image, landmarks)\n",
    "\n",
    "    # Display the image.\n",
    "    cv2.imshow('Image', image)\n",
    "    cv2.waitKey(0)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Exception encountered when calling layer \"roi_pooling_layer_2\" (type ROIPoolingLayer).\n\nin user code:\n\n    File \"C:\\Users\\HP 840 G1\\AppData\\Local\\Temp\\ipykernel_9376\\3285425138.py\", line 68, in call  *\n        h_start = tf.cast(tf.math.floor(roi[1] + ph * bin_size_h), dtype=tf.int32)\n\n    TypeError: Input 'y' of 'AddV2' Op has type float64 that does not match type int32 of argument 'x'.\n\n\nCall arguments received by layer \"roi_pooling_layer_2\" (type ROIPoolingLayer):\n  • inputs=['tf.Tensor(shape=(None, 14, 14, 1024), dtype=float32)', 'tf.Tensor(shape=(2, 4), dtype=int32)']",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 200\u001b[0m\n\u001b[0;32m    197\u001b[0m     train(train_dataset, model, optimizer, num_epochs)\n\u001b[0;32m    199\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m--> 200\u001b[0m     main()\n",
      "Cell \u001b[1;32mIn[13], line 183\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    178\u001b[0m rois \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mconstant([[\u001b[39m20\u001b[39m, \u001b[39m30\u001b[39m, \u001b[39m140\u001b[39m, \u001b[39m180\u001b[39m], [\u001b[39m100\u001b[39m, \u001b[39m120\u001b[39m, \u001b[39m220\u001b[39m, \u001b[39m240\u001b[39m]], dtype\u001b[39m=\u001b[39mtf\u001b[39m.\u001b[39mint32)  \u001b[39m# Ensure int32\u001b[39;00m\n\u001b[0;32m    181\u001b[0m \u001b[39m# Build ROI Pooling layer\u001b[39;00m\n\u001b[0;32m    182\u001b[0m \u001b[39m#roi_pooling_layer = ROIPoolingLayer(pooled_height, pooled_width)\u001b[39;00m\n\u001b[1;32m--> 183\u001b[0m roi_pooled_features \u001b[39m=\u001b[39m ROIPoolingLayer(pooled_height, pooled_width)([backbone_output, rois])  \u001b[39m# Implement ROIPoolingLayer\u001b[39;00m\n\u001b[0;32m    184\u001b[0m cls_output, reg_output \u001b[39m=\u001b[39m build_fast_rcnn(roi_pooled_features, num_classes)\n\u001b[0;32m    185\u001b[0m model \u001b[39m=\u001b[39m Model(inputs\u001b[39m=\u001b[39minput_layer, outputs\u001b[39m=\u001b[39m[rpn_cls, rpn_reg, cls_output, reg_output])\n",
      "File \u001b[1;32mc:\\Program Files\\Python311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mC:\\Users\\HP840G~1\\AppData\\Local\\Temp\\__autograph_generated_fileddbiyz4f.py:64\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m     62\u001b[0m pooled_value \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mUndefined(\u001b[39m'\u001b[39m\u001b[39mpooled_value\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     63\u001b[0m i \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mUndefined(\u001b[39m'\u001b[39m\u001b[39mi\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m---> 64\u001b[0m ag__\u001b[39m.\u001b[39;49mfor_stmt(ag__\u001b[39m.\u001b[39;49mconverted_call(ag__\u001b[39m.\u001b[39;49mld(\u001b[39mrange\u001b[39;49m), (ag__\u001b[39m.\u001b[39;49mconverted_call(ag__\u001b[39m.\u001b[39;49mld(tf)\u001b[39m.\u001b[39;49mshape, (ag__\u001b[39m.\u001b[39;49mld(rois),), \u001b[39mNone\u001b[39;49;00m, fscope)[\u001b[39m0\u001b[39;49m],), \u001b[39mNone\u001b[39;49;00m, fscope), \u001b[39mNone\u001b[39;49;00m, loop_body_2, get_state_2, set_state_2, (), {\u001b[39m'\u001b[39;49m\u001b[39miterate_names\u001b[39;49m\u001b[39m'\u001b[39;49m: \u001b[39m'\u001b[39;49m\u001b[39mi\u001b[39;49m\u001b[39m'\u001b[39;49m})\n\u001b[0;32m     65\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     66\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Users\\HP840G~1\\AppData\\Local\\Temp\\__autograph_generated_fileddbiyz4f.py:53\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call.<locals>.loop_body_2\u001b[1;34m(itr_2)\u001b[0m\n\u001b[0;32m     51\u001b[0m         ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(pooled_features)\u001b[39m.\u001b[39mappend, (ag__\u001b[39m.\u001b[39mld(pooled_value),), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[0;32m     52\u001b[0m     ag__\u001b[39m.\u001b[39mfor_stmt(ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(\u001b[39mrange\u001b[39m), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39mpooled_width,), \u001b[39mNone\u001b[39;00m, fscope), \u001b[39mNone\u001b[39;00m, loop_body, get_state, set_state, (), {\u001b[39m'\u001b[39m\u001b[39miterate_names\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mpw\u001b[39m\u001b[39m'\u001b[39m})\n\u001b[1;32m---> 53\u001b[0m ag__\u001b[39m.\u001b[39;49mfor_stmt(ag__\u001b[39m.\u001b[39;49mconverted_call(ag__\u001b[39m.\u001b[39;49mld(\u001b[39mrange\u001b[39;49m), (ag__\u001b[39m.\u001b[39;49mld(\u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49mpooled_height,), \u001b[39mNone\u001b[39;49;00m, fscope), \u001b[39mNone\u001b[39;49;00m, loop_body_1, get_state_1, set_state_1, (), {\u001b[39m'\u001b[39;49m\u001b[39miterate_names\u001b[39;49m\u001b[39m'\u001b[39;49m: \u001b[39m'\u001b[39;49m\u001b[39mph\u001b[39;49m\u001b[39m'\u001b[39;49m})\n",
      "File \u001b[1;32mC:\\Users\\HP840G~1\\AppData\\Local\\Temp\\__autograph_generated_fileddbiyz4f.py:52\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call.<locals>.loop_body_2.<locals>.loop_body_1\u001b[1;34m(itr_1)\u001b[0m\n\u001b[0;32m     50\u001b[0m     pooled_value \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(tf)\u001b[39m.\u001b[39mreduce_max, (ag__\u001b[39m.\u001b[39mld(pooled_region),), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[0;32m     51\u001b[0m     ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(pooled_features)\u001b[39m.\u001b[39mappend, (ag__\u001b[39m.\u001b[39mld(pooled_value),), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[1;32m---> 52\u001b[0m ag__\u001b[39m.\u001b[39;49mfor_stmt(ag__\u001b[39m.\u001b[39;49mconverted_call(ag__\u001b[39m.\u001b[39;49mld(\u001b[39mrange\u001b[39;49m), (ag__\u001b[39m.\u001b[39;49mld(\u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49mpooled_width,), \u001b[39mNone\u001b[39;49;00m, fscope), \u001b[39mNone\u001b[39;49;00m, loop_body, get_state, set_state, (), {\u001b[39m'\u001b[39;49m\u001b[39miterate_names\u001b[39;49m\u001b[39m'\u001b[39;49m: \u001b[39m'\u001b[39;49m\u001b[39mpw\u001b[39;49m\u001b[39m'\u001b[39;49m})\n",
      "File \u001b[1;32mC:\\Users\\HP840G~1\\AppData\\Local\\Temp\\__autograph_generated_fileddbiyz4f.py:45\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call.<locals>.loop_body_2.<locals>.loop_body_1.<locals>.loop_body\u001b[1;34m(itr)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mloop_body\u001b[39m(itr):\n\u001b[0;32m     44\u001b[0m     pw \u001b[39m=\u001b[39m itr\n\u001b[1;32m---> 45\u001b[0m     h_start \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(tf)\u001b[39m.\u001b[39mcast, (ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(tf)\u001b[39m.\u001b[39mmath\u001b[39m.\u001b[39mfloor, (ag__\u001b[39m.\u001b[39;49mld(roi)[\u001b[39m1\u001b[39;49m] \u001b[39m+\u001b[39;49m ag__\u001b[39m.\u001b[39;49mld(ph) \u001b[39m*\u001b[39;49m ag__\u001b[39m.\u001b[39;49mld(bin_size_h),), \u001b[39mNone\u001b[39;00m, fscope),), \u001b[39mdict\u001b[39m(dtype\u001b[39m=\u001b[39mag__\u001b[39m.\u001b[39mld(tf)\u001b[39m.\u001b[39mint32), fscope)\n\u001b[0;32m     46\u001b[0m     h_end \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(tf)\u001b[39m.\u001b[39mcast, (ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(tf)\u001b[39m.\u001b[39mmath\u001b[39m.\u001b[39mceil, (ag__\u001b[39m.\u001b[39mld(roi)[\u001b[39m1\u001b[39m] \u001b[39m+\u001b[39m (ag__\u001b[39m.\u001b[39mld(ph) \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m*\u001b[39m ag__\u001b[39m.\u001b[39mld(bin_size_h),), \u001b[39mNone\u001b[39;00m, fscope),), \u001b[39mdict\u001b[39m(dtype\u001b[39m=\u001b[39mag__\u001b[39m.\u001b[39mld(tf)\u001b[39m.\u001b[39mint32), fscope)\n\u001b[0;32m     47\u001b[0m     w_start \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(tf)\u001b[39m.\u001b[39mcast, (ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(tf)\u001b[39m.\u001b[39mmath\u001b[39m.\u001b[39mfloor, (ag__\u001b[39m.\u001b[39mld(roi)[\u001b[39m0\u001b[39m] \u001b[39m+\u001b[39m ag__\u001b[39m.\u001b[39mld(pw) \u001b[39m*\u001b[39m ag__\u001b[39m.\u001b[39mld(bin_size_w),), \u001b[39mNone\u001b[39;00m, fscope),), \u001b[39mdict\u001b[39m(dtype\u001b[39m=\u001b[39mag__\u001b[39m.\u001b[39mld(tf)\u001b[39m.\u001b[39mint32), fscope)\n",
      "\u001b[1;31mTypeError\u001b[0m: Exception encountered when calling layer \"roi_pooling_layer_2\" (type ROIPoolingLayer).\n\nin user code:\n\n    File \"C:\\Users\\HP 840 G1\\AppData\\Local\\Temp\\ipykernel_9376\\3285425138.py\", line 68, in call  *\n        h_start = tf.cast(tf.math.floor(roi[1] + ph * bin_size_h), dtype=tf.int32)\n\n    TypeError: Input 'y' of 'AddV2' Op has type float64 that does not match type int32 of argument 'x'.\n\n\nCall arguments received by layer \"roi_pooling_layer_2\" (type ROIPoolingLayer):\n  • inputs=['tf.Tensor(shape=(None, 14, 14, 1024), dtype=float32)', 'tf.Tensor(shape=(2, 4), dtype=int32)']"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.layers import Input, Conv2D, GlobalAveragePooling2D, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Step 1: Build the Backbone CNN (ResNet50 as an example)\n",
    "def build_backbone(input_shape):\n",
    "    base_model = ResNet50(weights='imagenet', include_top=False, input_tensor=Input(shape=input_shape))\n",
    "    intermediate_layer = base_model.get_layer('conv4_block6_out').output\n",
    "    return intermediate_layer\n",
    "\n",
    "# Step 2: Anchor Generation and RPN\n",
    "def generate_anchor_boxes(base_size, aspect_ratios, scales):\n",
    "    anchor_boxes = []\n",
    "    for aspect_ratio in aspect_ratios:\n",
    "        for scale in scales:\n",
    "            width = base_size * np.sqrt(scale * aspect_ratio)\n",
    "            height = base_size * np.sqrt(scale / aspect_ratio)\n",
    "            anchor_boxes.append((width, height))\n",
    "    return np.array(anchor_boxes)\n",
    "\n",
    "def generate_anchors(feature_map_size, anchor_scale, aspect_ratios):\n",
    "    base_anchor_size = anchor_scale  # You can adjust this value\n",
    "    anchors = []\n",
    "    for y in range(feature_map_size[0]):\n",
    "        for x in range(feature_map_size[1]):\n",
    "            center_x = (x + 0.5) / feature_map_size[1]\n",
    "            center_y = (y + 0.5) / feature_map_size[0]\n",
    "            for aspect_ratio in aspect_ratios:\n",
    "                width = base_anchor_size * np.sqrt(aspect_ratio)\n",
    "                height = base_anchor_size / np.sqrt(aspect_ratio)\n",
    "                anchors.append([center_x, center_y, width, height])\n",
    "    return np.array(anchors)\n",
    "\n",
    "def build_rpn(input_layer, num_anchors):\n",
    "    rpn_conv = Conv2D(256, (3, 3), activation='relu', padding='same')(input_layer)\n",
    "    rpn_cls = Conv2D(num_anchors, (1, 1), activation='sigmoid', name='rpn_cls')(rpn_conv)\n",
    "    rpn_reg = Conv2D(num_anchors * 4, (1, 1), activation='linear', name='rpn_reg')(rpn_conv)\n",
    "    return rpn_cls, rpn_reg\n",
    "\n",
    "# Step 3: ROI Pooling (or ROI Align)\n",
    "class ROIPoolingLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, pooled_height, pooled_width, **kwargs):\n",
    "        super(ROIPoolingLayer, self).__init__(**kwargs)\n",
    "        self.pooled_height = pooled_height\n",
    "        self.pooled_width = pooled_width\n",
    "\n",
    "    def call(self, inputs):\n",
    "        feature_maps, rois = inputs\n",
    "        feature_maps = tf.cast(feature_maps, tf.float32)\n",
    "\n",
    "        # Compute ROI dimensions and pooling bin sizes\n",
    "        roi_height = rois[:, 3] - rois[:, 1]\n",
    "        roi_width = rois[:, 2] - rois[:, 0]\n",
    "        bin_size_h = roi_height / self.pooled_height\n",
    "        bin_size_w = roi_width / self.pooled_width\n",
    "\n",
    "        # Initialize pooled feature map\n",
    "        pooled_features = []\n",
    "\n",
    "        # Apply ROI Pooling for each ROI\n",
    "        for i in range(tf.shape(rois)[0]):\n",
    "            roi = rois[i]\n",
    "            for ph in range(self.pooled_height):\n",
    "                for pw in range(self.pooled_width):\n",
    "                    h_start = tf.cast(tf.math.floor(roi[1] + ph * bin_size_h), dtype=tf.int32)\n",
    "                    h_end = tf.cast(tf.math.ceil(roi[1] + (ph + 1) * bin_size_h), dtype=tf.int32)\n",
    "                    w_start = tf.cast(tf.math.floor(roi[0] + pw * bin_size_w), dtype=tf.int32)\n",
    "                    w_end = tf.cast(tf.math.ceil(roi[0] + (pw + 1) * bin_size_w), dtype=tf.int32)\n",
    "                    pooled_region = feature_maps[h_start:h_end, w_start:w_end]\n",
    "                    pooled_value = tf.reduce_max(pooled_region)\n",
    "                    pooled_features.append(pooled_value)\n",
    "\n",
    "        return tf.reshape(pooled_features, (-1, self.pooled_height, self.pooled_width, tf.shape(feature_maps)[-1]))\n",
    "    \n",
    "\n",
    "# Step 4: Fast R-CNN Head\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense\n",
    "\n",
    "def build_fast_rcnn(input_layer, num_classes):\n",
    "    # Global Average Pooling\n",
    "    pooled_features = GlobalAveragePooling2D()(input_layer)\n",
    "\n",
    "    # Fully connected layers\n",
    "    fc1 = Dense(256, activation='relu')(pooled_features)\n",
    "    fc2 = Dense(256, activation='relu')(fc1)\n",
    "\n",
    "    # Class prediction output\n",
    "    cls_output = Dense(num_classes, activation='softmax', name='cls_output')(fc2)\n",
    "\n",
    "    # Bounding box regression output\n",
    "    num_bbox_regression_values = num_classes * 4\n",
    "    reg_output = Dense(num_bbox_regression_values, activation='linear', name='reg_output')(fc2)\n",
    "\n",
    "    return cls_output, reg_output\n",
    "\n",
    "# Step 5: Loss Functions\n",
    "def rpn_classification_loss(y_true, y_pred):\n",
    "    rpn_cls_loss = tf.keras.losses.binary_crossentropy(y_true, y_pred)\n",
    "    return rpn_cls_loss\n",
    "\n",
    "def rpn_regression_loss(y_true, y_pred, positive_indices):\n",
    "    diff = y_true - y_pred\n",
    "    absolute_diff = tf.abs(diff)\n",
    "    mask = tf.cast(tf.less(absolute_diff, 1.0), tf.float32)\n",
    "    rpn_reg_loss = tf.reduce_sum(mask * (0.5 * diff**2) + (1 - mask) * (absolute_diff - 0.5))\n",
    "    return rpn_reg_loss\n",
    "\n",
    "def fast_rcnn_classification_loss(y_true, y_pred):\n",
    "    cls_loss = tf.keras.losses.categorical_crossentropy(y_true, y_pred)\n",
    "    return cls_loss\n",
    "\n",
    "def fast_rcnn_regression_loss(y_true, y_pred, positive_indices):\n",
    "    diff = y_true - y_pred\n",
    "    absolute_diff = tf.abs(diff)\n",
    "    mask = tf.cast(tf.less(absolute_diff, 1.0), tf.float32)\n",
    "    reg_loss = tf.reduce_sum(mask * (0.5 * diff**2) + (1 - mask) * (absolute_diff - 0.5))\n",
    "    return reg_loss\n",
    "\n",
    "# Step 6: Training Loop\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "def train_step(images, rpn_cls_true, rpn_reg_true, cls_true, reg_true, model, optimizer):\n",
    "    with tf.GradientTape() as tape:\n",
    "        rpn_cls_pred, rpn_reg_pred, cls_pred, reg_pred = model(images, training=True)\n",
    "\n",
    "        rpn_cls_loss = rpn_classification_loss(rpn_cls_true, rpn_cls_pred)\n",
    "        rpn_reg_loss = rpn_regression_loss(rpn_reg_true, rpn_reg_pred, positive_indices_rpn)\n",
    "        cls_loss = fast_rcnn_classification_loss(cls_true, cls_pred)\n",
    "        reg_loss = fast_rcnn_regression_loss(reg_true, reg_pred, positive_indices_rcnn)\n",
    "\n",
    "        total_loss = rpn_cls_loss + rpn_reg_loss + cls_loss + reg_loss\n",
    "\n",
    "    gradients = tape.gradient(total_loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    return total_loss\n",
    "\n",
    "def train(dataset, model, optimizer, epochs):\n",
    "    for epoch in range(epochs):\n",
    "        for batch_images, batch_rpn_cls_true, batch_rpn_reg_true, batch_cls_true, batch_reg_true in dataset:\n",
    "            loss = train_step(batch_images, batch_rpn_cls_true, batch_rpn_reg_true, batch_cls_true, batch_reg_true, model, optimizer)\n",
    "            print(f'Epoch {epoch + 1}, Batch Loss: {loss.numpy():.4f}')\n",
    "        # Optionally, evaluate the model on validation set here\n",
    "\n",
    "\n",
    "def main():\n",
    "    # ... Load and preprocess your dataset ...\n",
    "\n",
    "    input_shape = (224, 224, 3)\n",
    "    num_classes = 4\n",
    "    num_anchors = 9\n",
    "    num_epochs = 10\n",
    "\n",
    "    # Build the Faster R-CNN model\n",
    "    input_layer = tf.keras.layers.Input(shape=input_shape)\n",
    "    backbone_output = build_backbone(input_shape)\n",
    "    rpn_cls, rpn_reg = build_rpn(backbone_output, num_anchors)\n",
    "\n",
    "\n",
    "    pooled_height = 7  # Pooled feature map height\n",
    "    pooled_width = 7   # Pooled feature map width\n",
    "    #feature_maps_shape = (None, None, None, 256)  # Example feature map shape\n",
    "    #rois_shape = (None, 4)  # Example ROIs shape\n",
    "\n",
    "    # Create example inputs (feature maps and ROIs)\n",
    "    #feature_maps = tf.random.normal(feature_maps_shape)\n",
    "    rois = tf.constant([[20, 30, 140, 180], [100, 120, 220, 240]])\n",
    "\n",
    "    feature_maps_shape = (2, 14, 14, 1024)  # Example feature map shape\n",
    "    rois_shape = (None, 4)  # Example ROIs shape\n",
    "\n",
    "    # Create example inputs (feature maps and ROIs)\n",
    "    feature_maps = tf.random.normal(feature_maps_shape, dtype=tf.float32)  # Convert to float32\n",
    "    rois = tf.constant([[20, 30, 140, 180], [100, 120, 220, 240]], dtype=tf.int32)  # Ensure int32\n",
    "\n",
    "\n",
    "    # Build ROI Pooling layer\n",
    "    #roi_pooling_layer = ROIPoolingLayer(pooled_height, pooled_width)\n",
    "    roi_pooled_features = ROIPoolingLayer(pooled_height, pooled_width)([backbone_output, rois])  # Implement ROIPoolingLayer\n",
    "    cls_output, reg_output = build_fast_rcnn(roi_pooled_features, num_classes)\n",
    "    model = Model(inputs=input_layer, outputs=[rpn_cls, rpn_reg, cls_output, reg_output])\n",
    "\n",
    "    # Compile the model and optimizer\n",
    "    optimizer = Adam(learning_rate=0.001)\n",
    "    model.compile(optimizer=optimizer, loss=[rpn_classification_loss, rpn_regression_loss,\n",
    "                                             fast_rcnn_classification_loss, fast_rcnn_regression_loss])\n",
    "\n",
    "    # Load and preprocess your dataset\n",
    "    # train_dataset = DataLoader.load_train_dataset()  # Implement your data loading utilities\n",
    "    (x_train,y_train),(_,_) = datasets\n",
    "    train_dataset = (x_train,y_train)\n",
    "    # Start training\n",
    "    train(train_dataset, model, optimizer, num_epochs)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load segmented mask and original image\n",
    "segmented_mask = cv2.imread('C:/Users/HP 840 G1/Documents/VS Code Projects/Workspace Learning/DataScience/Jupyter Notebook/Thesis_CSE\\Masked.png', cv2.IMREAD_GRAYSCALE)\n",
    "original_image = cv2.imread('C:/Users/HP 840 G1/Documents/VS Code Projects/Workspace Learning/DataScience/Jupyter Notebook/Thesis_CSE/original1.png')\n",
    "\n",
    "# Find contours in the mask\n",
    "contours, _ = cv2.findContours(segmented_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "# Draw bounding boxes on the original image\n",
    "for contour in contours:\n",
    "    x, y, w, h = cv2.boundingRect(contour)\n",
    "    cv2.rectangle(original_image, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "\n",
    "# Display the result\n",
    "cv2.imshow('Image with Bounding Boxes', original_image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
