{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Importing Libraries <a name=\"ImportingLibraries\"></a>","metadata":{}},{"cell_type":"code","source":"!pip install tensorflow==2.3.0 -q","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nimport os\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom kaggle_datasets import KaggleDatasets\nimport matplotlib.pyplot as plt\nimport PIL\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Device:',tpu.master())\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_systems(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nexcept:\n    strategy = tf.distribute.get_strategy()\nprint('Number of replicas:', strategy.num_replicas_in_sync)\n\nprint(tf.__version__)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loading Dataset <a name=\"LoadingDataset\"></a>","metadata":{}},{"cell_type":"code","source":"#!/usr/bin/python\n\nimport os, sys\n\n# Create new Train and val folders\n\nbase_dir = 'kaggle/input/RiceLeafs'\ntrain_path = '/kaggle/input/RiceLeafs/train'\nval_path = 'kaggle/input/RiceLeafs/validation/'\n\ncolumn_names = os.listdir(train_path)\nfor i in column_names:\n    os.makedirs(f'../kaggle/output/train/{i}')\n    os.makedirs(f'../kaggle/output/validation/{i}')\n\nout_path = '../kaggle/output/train/'\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Resizing Image  <a name=\"Resize\"></a>","metadata":{}},{"cell_type":"code","source":"from PIL import Image\ndef resize(input_path,folder,column_name):\n    dirs = os.listdir(input_path)\n    for item in dirs:\n        item_path = input_path +'/' +item\n        if os.path.isfile(item_path):\n            #print('CHECK')\n            im = Image.open(item_path)\n\n            # Check whether the specified \n            # path exists or not \n            outpath = f'/kaggle/kaggle/output/{folder}/{column_name}'\n            temp_out_path = outpath+'/'+item\n            f, e = os.path.splitext(temp_out_path)\n\n            imResize = im.resize((150,150), Image.ANTIALIAS)\n            #print('CHECK 3')\n            imResize.save(f + '.jpg', 'JPEG', quality=90)\n\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_path = '../input/RiceLeafs/train/Healthy'\nfolder = 'train'\ncolumn_name = 'Healthy'\nresize(input_path,folder,column_name)\n\ninput_path = '../input/RiceLeafs/train/BrownSpot'\nfolder = 'train'\ncolumn_name = 'BrownSpot'\nresize(input_path,folder,column_name)\n\ninput_path = '../input/RiceLeafs/train/Hispa'\nfolder = 'train'\ncolumn_name = 'Hispa'\nresize(input_path,folder,column_name)\n\ninput_path = '../input/RiceLeafs/train/LeafBlast'\nfolder = 'train'\ncolumn_name = 'LeafBlast'\nresize(input_path,folder,column_name)\n\nprint('Done with train resizing')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## VALIDATION\ninput_path = '../input/RiceLeafs/validation/Healthy'\nfolder = 'validation'\ncolumn_name = 'Healthy'\nresize(input_path,folder,column_name)\n\ninput_path = '../input/RiceLeafs/validation/BrownSpot'\nfolder = 'validation'\ncolumn_name = 'BrownSpot'\nresize(input_path,folder,column_name)\n\ninput_path = '../input/RiceLeafs/validation/Hispa'\nfolder = 'validation'\ncolumn_name = 'Hispa'\nresize(input_path,folder,column_name)\n\ninput_path = '../input/RiceLeafs/validation/LeafBlast'\nfolder = 'validation'\ncolumn_name = 'LeafBlast'\nresize(input_path,folder,column_name)\n\nprint('Done with Validation resizing')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.path.exists('/kaggle/kaggle/output/validation/Healthy/')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.path.exists('/kaggle/kaggle/output/train/')\nos.path.exists('/kaggle/kaggle/output/validation/')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.listdir('/kaggle/kaggle/output/train/BrownSpot/')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_dir = os.path.join(os.path.dirname('/kaggle/kaggle/'), 'output')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training and Validation Split  <a name=\"Split\"></a>","metadata":{}},{"cell_type":"code","source":"# Use this if you avoided the resizing\n#data_dir = os.path.join(os.path.dirname('../input/'), 'riceleafs/RiceLeafs')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dir = os.path.join(data_dir, 'train')\ntrain_BrownSpot_dir = os.path.join(train_dir, 'BrownSpot')\ntrain_Healthy_dir = os.path.join(train_dir, 'Healthy')\ntrain_Hispa_dir = os.path.join(train_dir, 'Hispa')\ntrain_LeafBlast_dir = os.path.join(train_dir, 'LeafBlast')\n\n\nvalidation_dir = os.path.join(data_dir, 'validation')\nvalidation_BrownSpot_dir = os.path.join(validation_dir, 'BrownSpot')\nvalidation_Healthy_dir = os.path.join(validation_dir, 'Healthy')\nvalidation_Hispa_dir = os.path.join(validation_dir, 'Hispa')\nvalidation_LeafBlast_dir = os.path.join(validation_dir, 'LeafBlast')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_BrownSpot_names = os.listdir(train_BrownSpot_dir)\nprint(train_BrownSpot_names[:10])\n\ntrain_Healthy_names =  os.listdir(train_Healthy_dir)\nprint(train_Healthy_names[:10])\n\ntrain_Hispa_names = os.listdir(train_Hispa_dir)\nprint(train_Hispa_names[:10])\n\ntrain_LeafBlast_names =  os.listdir(train_LeafBlast_dir)\nprint(train_LeafBlast_names[:10])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Image Count <a name=\"ImageCount\"></a>","metadata":{}},{"cell_type":"code","source":"\nimport time\nimport os\nfrom os.path import exists\n\ndef count(dir, counter=0):\n    \"returns number of files in dir and subdirs\"\n    for pack in os.walk(dir):\n        for f in pack[2]:\n            counter += 1\n    return dir + \" : \" + str(counter) + \" files\"\n\nprint('total images for training :', count(train_dir))\nprint('total images for validation :', count(validation_dir))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Viewing Images  <a name=\"ViewingImages\"></a>","metadata":{}},{"cell_type":"code","source":"%matplotlib inline\n\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\n\n\n# Parameters for our graph; we'll outpu images in a 4x4 configuration\nnrows = 4\nncols = 4\n\n# for iternating over images\npic_index = 0","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Set up matplotlib fig, and size it to fit 4x4 pics\n\nfig = plt.gcf()\nfig.set_size_inches(ncols *4, nrows*4)\n\npic_index += 4\nnext_BrownSpot_pix = [os.path.join(train_BrownSpot_dir, fname)\n                for fname in train_BrownSpot_names[pic_index-4:pic_index]]\n\nnext_Healthy_pix = [os.path.join(train_Healthy_dir, fname)\n                for fname in train_Healthy_names[pic_index-4:pic_index]]\n\nnext_Hispa_pix = [os.path.join(train_Hispa_dir, fname)\n                for fname in train_Hispa_names[pic_index-4:pic_index]]\n\nnext_LeafBlast_pix = [os.path.join(train_LeafBlast_dir, fname)\n                for fname in train_LeafBlast_names[pic_index-4:pic_index]]\n\nfor i, img_path in enumerate(next_BrownSpot_pix + next_Healthy_pix + next_Hispa_pix + next_LeafBlast_pix):\n  # Set up subplot; subplot indices start at 1\n  sp = plt.subplot(nrows,ncols,i +1)\n  #sp.axis('Off') # Don't show axes (or gridlines)\n\n  img = mpimg.imread(img_path)\n  plt.imshow(img)\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Generators <a name=\"DataAugAndGen\"></a>","metadata":{}},{"cell_type":"code","source":"AUTOTUNE = tf.data.experimental.AUTOTUNE\nEPOCHS = 100\nIMAGE_SIZE = (150, 150)\nBATCH_SIZE = 32 * strategy.num_replicas_in_sync","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(strategy.num_replicas_in_sync)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.preprocessing import image_dataset_from_directory\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\n# train_ds = ImageDataGenerator(\n# rescale = 1./255,\n# width_shift_range= 0.2,\n# height_shift_range = 0.2,\n# shear_range = 0.4,\n# rotation_range = 40,\n# horizontal_flip = True,\n# fill_mode = 'nearest')\n\ntraining_generator = image_dataset_from_directory(\n    train_dir,\n    validation_split = 0.2,\n    subset = 'training',\n    seed = 220,\n    image_size = IMAGE_SIZE,\n    batch_size = BATCH_SIZE,\n    #class_mode = 'categorical'\n)\n\n# validation_ds = ImageDataGenerator(\n#             rescale = 1./255)\n\nvalidation_generator = image_dataset_from_directory(\n        validation_dir,\n        validation_split = 0.2,\n        subset = 'validation',\n        seed = 220,\n        batch_size = BATCH_SIZE,\n        image_size = IMAGE_SIZE,\n        )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(10, 10))\nfor images, labels in training_generator.take(1):\n  for i in range(9):\n    ax = plt.subplot(3, 3, i + 1)\n    plt.imshow(images[i].numpy().astype(\"uint8\"))\n    plt.title(training_generator.class_names[labels[i]])\n    plt.axis(\"off\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class_names = os.listdir(train_dir)\n\nprint(class_names)\n\ntraining_generator.class_names = class_names\nvalidation_generator.class_names = class_names\n\nNUM_CLASSES = len(class_names)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"# Feature Engineering","metadata":{}},{"cell_type":"code","source":"def one_hot_label(image, label):\n    label = tf.one_hot(label, NUM_CLASSES)\n    return image, label\n\ntrain_ds = training_generator.map(one_hot_label, num_parallel_calls=AUTOTUNE)\nval_ds = validation_generator.map(one_hot_label, num_parallel_calls=AUTOTUNE)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_ds = train_ds.cache().prefetch(buffer_size = AUTOTUNE)\nval_ds = val_ds.cache().prefetch(buffer_size = AUTOTUNE)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Build the Model","metadata":{}},{"cell_type":"code","source":"def conv_block(filters):\n    block = tf.keras.Sequential([\n        tf.keras.layers.SeparableConv2D(filters,3,activation = 'relu',padding = 'same'),\n        tf.keras.layers.SeparableConv2D(filters,3,activation = 'relu',padding = 'same'),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.MaxPool2D()\n    ])\n    return block","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def dense_block(units,dropout_rate):\n    block = tf.keras.Sequential([\n        tf.keras.layers.Dense(units,activation = 'relu'),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.Dropout(dropout_rate)\n    ])\n    return block","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def build_model():\n    model = tf.keras.Sequential([\n        tf.keras.Input(shape=(*IMAGE_SIZE,3)),\n        tf.keras.layers.Conv2D(16,3,activation = 'relu', padding = 'same'),\n        tf.keras.layers.MaxPool2D(),\n        \n        conv_block(32),\n        conv_block(64),\n        \n        #conv_block(128),\n        tf.keras.layers.Dropout(0.2),\n        \n        conv_block(256),\n        tf.keras.layers.Dropout(0.2),\n        tf.keras.layers.Flatten(),\n        \n        #dense_block(512,0.7),\n        dense_block(128,0.5),\n        dense_block(64,0.3),\n        \n        tf.keras.layers.Dense(NUM_CLASSES,activation = 'softmax')\n            ])\n    return model\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with strategy.scope():\n    model = build_model()\n    \n    METRICS = [tf.keras.metrics.AUC(name='auc')]\n    \n    model.compile(\n    optimizer = 'adam',\n    loss = tf.losses.CategoricalCrossentropy(),\n    metrics = METRICS\n    )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training the Model","metadata":{}},{"cell_type":"code","source":"def exponential_decay(lr0,s):\n    def exponential_decay_fn(epoch):\n        return lr0 * 0.1 **(epoch/s) \n    return exponential_decay_fn\n\n\nexponential_decay_fn = exponential_decay(0.001,20)\n\nlr_scheduler = tf.keras.callbacks.LearningRateScheduler(exponential_decay_fn)\n\ncheckpoint_cb = tf.keras.callbacks.ModelCheckpoint('Alzheimer_model.h5',\n                                                  save_best_only = True)\n\nearly_stopping_cb = tf.keras.callbacks.EarlyStopping(patience = 10,\n                                                    restore_best_weights = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model.fit(\n    train_ds,\n    validation_data=val_ds,\n    callbacks=[checkpoint_cb, early_stopping_cb, lr_scheduler],\n    epochs=EPOCHS\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(1, 2, figsize=(20, 3))\nax = ax.ravel()\n\nfor i, met in enumerate(['auc', 'loss']):\n    ax[i].plot(history.history[met])\n    ax[i].plot(history.history['val_' + met])\n    ax[i].set_title('Model {}'.format(met))\n    ax[i].set_xlabel('epochs')\n    ax[i].set_ylabel(met)\n    ax[i].legend(['train', 'val'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### TensorFlow Hub Dataset\n- [EfficientNet B7](https://tfhub.dev/tensorflow/efficientnet/b7/feature-vector/1)","metadata":{}}]}